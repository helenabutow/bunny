receivers:
  # Data sources: metrics
  hostmetrics:
    scrapers:
      # these aren't supported by otelcol right now
      # cpu:
      # disk:
      filesystem:
      load:
      memory:
      network:
      # this is pretty noisy when processes die mid-scrape
      # process:
      processes:
      paging:

  # Data sources: traces, metrics, logs
  otlp:
    protocols:
      grpc:
      http:

  # Data sources: metrics
  prometheus:
    config:
      scrape_configs:
        - job_name: otel-collector
          scrape_interval: 1s
          static_configs:
            - targets: [localhost:1312]

processors:
  batch:
    send_batch_max_size: 10000
    timeout: 0s

exporters:
  # Data sources: traces, metrics, logs
  # NOTE: Prior to v0.86.0 use `logging` instead of `debug`
  debug:
    verbosity: detailed

  # Data sources: traces, metrics, logs
  otlp:
    endpoint: localhost:30002

  # Data sources: traces, metrics
  # for Mimir, it looks like we would do something like this:
  # otlphttp:
  #   endpoint: http://localhost:30002

  # Data sources: metrics
  # see: https://grafana.com/docs/mimir/latest/configure/configure-otel-collector/#remote-write
  # also: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/exporter/prometheusremotewriteexporter/README.md
  prometheusremotewrite:
    endpoint: http://localhost:30001/api/v1/push
    # disabling this as we already use the batch processor and don't need more delay
    remote_write_queue:
      enabled: false
    # disabling this as it doesn't provide any useful info
    target_info:
      enabled: false

extensions:
  health_check:
  pprof:
  zpages:

service:
  extensions: [health_check, pprof, zpages]
  pipelines:
    # traces:
    #   receivers: [otlp]
    #   processors: [batch]
    #   exporters: [otlp]
    metrics:
      receivers: [hostmetrics, prometheus]
      processors: [batch]
      exporters: [prometheusremotewrite]
    # logs:
    #   receivers: [otlp]
    #   processors: [batch]
    #   exporters: [otlp]